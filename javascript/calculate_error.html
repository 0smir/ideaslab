<!DOCTYPE html>
<html>
<head>
	<meta charset="UTF-8">
	<title>Error</title>
</head>

<script>
	var a = 0.1;
	var b = 0.2;

	var result = (a * 10 + b * 10)/10;
		if (result == 0.3) {
			console.log(true);
		}else{
			console.log(false);
		}
	
</script>


<body>
	<p>
		Всё дело в том, что в стандарте IEEE 754 на число выделяется ровно 8 байт(=64 бита), не больше и не меньше.
	</p>
	<p>
		Число 0.1 (одна десятая) записывается просто в десятичном формате. Но в двоичной системе счисления это бесконечная дробь, так как единица единицаа десять в двоичной системе так просто не делится. Также бесконечной дробью является 0.2 (=2/10).
	</p>
	<p>
		Из 64 бит, отведённых на число, сами цифры числа занимают до 52 бит, остальные 11 бит хранят позицию десятичной точки и один бит – знак. Так что если 52 бит не хватает на цифры, то при записи пропадут младшие разряды.
	</p>
</body>
</html>